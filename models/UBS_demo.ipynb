{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn import tree\n",
    "from IPython.display import SVG\n",
    "from graphviz import Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load IRS csv\n",
    "# df_2016 = pd.read_csv('https://www.irs.gov/pub/irs-soi/16zpallagi.csv')\n",
    "# df_2012 = pd.read_csv(\"https://www.irs.gov/pub/irs-soi/12zpallagi.csv')\n",
    "df_2016 = pd.read_csv(\"16zpallagi.csv\")\n",
    "df_2012 = pd.read_csv(\"12zpallagi.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter based on highest income bracket\n",
    "df_2016 = df_2016.loc[df_2016['agi_stub'] == 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# exclude zipcode 0 and 99999 (not sure what these are)\n",
    "df_2016 = df_2016[~df_2016.zipcode.isin([0, 99999])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_2012 = df_2012.loc[df_2012['AGI_STUB'] == 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_2012 = df_2012[~df_2012.zipcode.isin([0, 99999])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_2012 = df_2012[['zipcode', 'N1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_2012.columns = ['zipcode', 'N1_2012']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# join the two tables on zip code\n",
    "df = df_2016.join(df_2012.set_index('zipcode'), on='zipcode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[~df['N1_2012'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load UBS branches csv\n",
    "branch = pd.read_csv(\"UBS Branches.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# join the two tables on zip code\n",
    "ubs_data = branch.join(df.set_index('zipcode'), on='Zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# table of missing zip codes in IRS dataset\n",
    "# missing_irs = ubs_data[ubs_data['STATE'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save missing table to csv\n",
    "# header = [\"City\", \"State\", \"Zip\"]\n",
    "# missing_irs.to_csv('missing_irs.csv', columns = header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create working table\n",
    "ubs_irs = ubs_data[ubs_data['STATE'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop irrelevant columns\n",
    "ubs_irs = ubs_irs.drop(['Zip', 'STATE', 'STATEFIPS', 'agi_stub'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save table to csv\n",
    "# ubs_irs.to_csv('ubs_irs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge City and State columns to create Location\n",
    "ubs_irs['Location'] = ubs_irs.apply (lambda row: row.City + \", \" + row.State,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# each row means there is an office at Location\n",
    "ubs_irs['UBS offices'] = ubs_irs.apply (lambda row: 1,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop extra columns\n",
    "ubs_irs = ubs_irs.drop(['City', 'State'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# group by Location and sum the stats\n",
    "group = ubs_irs.groupby('Location', as_index=False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save to csv\n",
    "# group.to_csv('ubs_irs_linear.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# exclude Location that has 0 stats\n",
    "group = group[group['N1_2012'] != 0]\n",
    "group = group[group['N1'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######## Jeremy's suggestions\n",
    "\n",
    "# Adjusted Gross Income / Number of Returns\n",
    "def ave_AGI (row):\n",
    "    return row.A00100 / row.N1\n",
    "\n",
    "# (Number of returns with ordinary divdends + Number of returns with qualified dividends) / Number of Returns\n",
    "def ave_div (row):\n",
    "    return (row.N00600 + row.N00650) / row.N1\n",
    "\n",
    "# Number of returns with business or professional net income / Number of Returns\n",
    "def ave_pro_income (row):\n",
    "    return row.N00900 / row.N1\n",
    "\n",
    "# Number of returns with self employed (Keogh) retirement plans / Number of Returns\n",
    "def ave_self_retired (row):\n",
    "    return row.N03300 / row.N1\n",
    "\n",
    "# Number of returns with itemized deductions / Number of Returns\n",
    "def ave_item_deduct (row):\n",
    "    return row.N04470 / row.N1\n",
    "\n",
    "# Number of returns with real estate deductions / Number of Returns\n",
    "def ave_RET (row):\n",
    "    return row.N18500 / row.N1\n",
    "\n",
    "# Number of returns with Alternative Minimum Tax / Number of Returns\n",
    "def ave_AMT (row):\n",
    "    return row.N09600 / row.N1\n",
    "\n",
    "# Number of returns with earned income credit / Number of Returns\n",
    "def ave_EIC (row):\n",
    "    return row.N59660 / row.N1\n",
    "\n",
    "# Number of returns with unemployment compensation / Number of Returns\n",
    "## Does this need to be negative?\n",
    "def ave_unemployment (row):\n",
    "    return row.N02300 / row.N1\n",
    "\n",
    "# Educator expenses amount / Number of returns with educator expenses \n",
    "def ave_edu_exp (row):\n",
    "    if row.N03220 == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return row.A03220 / row.N03220\n",
    "\n",
    "######## Charlie's suggestions\n",
    "\n",
    "# Total Tax Credit Amount / Adjusted Gross Income\n",
    "def tot_tax_cred (row):\n",
    "    return row.A07100 / row.A00100\n",
    "\t\n",
    "# Total Income Amount\n",
    "def tot_inc (row):\n",
    "    return row.A02650\n",
    "\t\n",
    "# Total Taxable Pensions and Annuity Amount\n",
    "def tot_pens (row):\n",
    "    return row.A01700\n",
    "\n",
    "# Total Charitable Contributions Amount\n",
    "def char_cont (row):\n",
    "    return row.A19700\n",
    "\t\n",
    "# Total Net Investment Income Tax\n",
    "def net_inv (row):\n",
    "    return row.A85300\n",
    "\n",
    "# Change in number of filings in top bracket \n",
    "def filings_change (row):\n",
    "    return row.N1 / row.N1_2012 - 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create new columns\n",
    "group['ave_div'] = group.apply (lambda row: ave_div (row),axis=1)\n",
    "group['ave_pro_income'] = group.apply (lambda row: ave_pro_income (row),axis=1)\n",
    "group['ave_self_retired'] = group.apply (lambda row: ave_self_retired (row),axis=1)\n",
    "group['ave_item_deduct'] = group.apply (lambda row: ave_item_deduct (row),axis=1)\n",
    "group['ave_RET'] = group.apply (lambda row: ave_RET (row),axis=1)\n",
    "group['ave_AMT'] = group.apply (lambda row: ave_AMT (row),axis=1)\n",
    "group['ave_EIC'] = group.apply (lambda row: ave_EIC (row),axis=1)\n",
    "group['ave_unemployment'] = group.apply (lambda row: ave_unemployment (row),axis=1)\n",
    "group['ave_edu_exp'] = group.apply (lambda row: ave_edu_exp (row),axis=1)\n",
    "\n",
    "group['tot_tax_cred'] = group.apply (lambda row: tot_tax_cred (row),axis=1)\n",
    "group['tot_inc'] = group.apply (lambda row: tot_inc (row),axis=1)\n",
    "group['tot_pens'] = group.apply (lambda row: tot_pens (row),axis=1)\n",
    "group['char_cont'] = group.apply (lambda row: char_cont (row),axis=1)\n",
    "group['net_inv'] = group.apply (lambda row: net_inv (row),axis=1)\n",
    "group['filings_change'] = group.apply (lambda row: filings_change (row),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create new working table that only uses the selected features\n",
    "data = group[['ave_div', 'ave_pro_income', 'ave_self_retired', 'ave_item_deduct', \n",
    "              'ave_RET', 'ave_AMT', 'ave_EIC', 'ave_unemployment', 'ave_edu_exp',\n",
    "              'tot_tax_cred', 'tot_inc', 'tot_pens', 'char_cont', 'net_inv', 'filings_change', 'UBS offices']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# high weight samples\n",
    "hw_data = data[data['UBS offices'] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# low weight samples\n",
    "lw_data = data[data['UBS offices'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# randomly split the low weight samples into train and test set\n",
    "X_train, X_test = train_test_split(lw_data, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add high weight samples to training data\n",
    "X_train = X_train.append(hw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop UBS offices column\n",
    "X_test = X_test.drop(['UBS offices'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create 10-fold for cross validation\n",
    "kf = KFold(n_splits=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Class SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# kernel can be linear, rbf, poly, sigmoid\n",
    "ocsvm = svm.OneClassSVM(nu=0.001, kernel='sigmoid', gamma='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fold 0] train: 98.36, cv: 100.00, test: 100.00\n",
      "[fold 1] train: 98.91, cv: 100.00, test: 96.20\n",
      "[fold 2] train: 97.81, cv: 100.00, test: 98.73\n",
      "[fold 3] train: 97.81, cv: 100.00, test: 98.73\n",
      "[fold 4] train: 98.37, cv: 100.00, test: 100.00\n",
      "[fold 5] train: 99.46, cv: 100.00, test: 100.00\n",
      "[fold 6] train: 97.83, cv: 100.00, test: 98.73\n",
      "[fold 7] train: 98.91, cv: 100.00, test: 100.00\n",
      "[fold 8] train: 97.83, cv: 100.00, test: 97.47\n",
      "[fold 9] train: 98.91, cv: 95.00, test: 100.00\n"
     ]
    }
   ],
   "source": [
    "# arrays to store model performance\n",
    "train_ocsvm = []\n",
    "cv_ocsvm = []\n",
    "test_ocsvm = []\n",
    "\n",
    "# split training set into 10 part, each time the model is trained on 9 parts and cross validated on the remaining part\n",
    "for k, (train_index, test_index) in enumerate(kf.split(X_train)):\n",
    "    X, X_cv = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "    # sample weights\n",
    "    sample_weights = X['UBS offices']\n",
    "    # drop UBS offices column\n",
    "    X = X.drop(['UBS offices'], axis=1)\n",
    "    X_cv = X_cv.drop(['UBS offices'], axis=1)\n",
    "    # scale to mean=0 and stdev=1 based on training set\n",
    "    scaler = preprocessing.StandardScaler().fit(X)\n",
    "    # apply scaling function on train, cross validation, and test set\n",
    "    X_train_n = scaler.transform(X)\n",
    "    X_cv_norm = scaler.transform(X_cv)\n",
    "    X_test_norm = scaler.transform(X_test)\n",
    "    # Demo model's sensitivity: modify weights\n",
    "    X['tot_inc'] = X.apply(lambda row: row.tot_inc * 2.0, axis=1)\n",
    "    # fit classifier on train set\n",
    "    ocsvm.fit(X_train_n, sample_weight = sample_weights)\n",
    "    # prediction on train set, cross validation, and test set (either 1 or -1)\n",
    "    y_train = ocsvm.predict(X_train_n)\n",
    "    y_cv = ocsvm.predict(X_cv_norm)\n",
    "    y_test = ocsvm.predict(X_test_norm)\n",
    "    # count the number of wrong classifications\n",
    "    train_error = y_train[y_train == -1].size\n",
    "    cv_error = y_cv[y_cv == -1].size\n",
    "    test_error = y_test[y_test == -1].size\n",
    "    # model accuracy\n",
    "    train_accuracy = 1 - train_error / y_train.size\n",
    "    cv_accuracy = 1 - cv_error / y_cv.size\n",
    "    test_accuracy = 1 - test_error / y_test.size\n",
    "    # print\n",
    "    train_ocsvm.append(train_accuracy)\n",
    "    cv_ocsvm.append(cv_accuracy)\n",
    "    test_ocsvm.append(test_accuracy)\n",
    "    print(\"[fold {0}] train: {1:.2f}, cv: {2:.2f}, test: {3:.2f}\".\n",
    "          format(k, 100 * train_accuracy, 100 * cv_accuracy, 100 * test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean score and the 95% confidence interval of the score estimate are hence given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 98.42 (+/- 1.14)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train accuracy: %0.2f (+/- %0.2f)\" % (100 * np.asarray(train_ocsvm).mean(), 100 * np.asarray(train_ocsvm).std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV accuracy: 99.50 (+/- 3.00)\n"
     ]
    }
   ],
   "source": [
    "print(\"CV accuracy: %0.2f (+/- %0.2f)\" % (100 * np.asarray(cv_ocsvm).mean(), 100 * np.asarray(cv_ocsvm).std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 98.99 (+/- 2.48)\n"
     ]
    }
   ],
   "source": [
    "print(\"Test accuracy: %0.2f (+/- %0.2f)\" % (100 * np.asarray(test_ocsvm).mean(), 100 * np.asarray(test_ocsvm).std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a model caller\n",
    "iso_f = IsolationForest(n_estimators=100,random_state=42, contamination=0.000, behaviour='new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fold 0] train: 100.00, cv: 100.00, test: 100.00\n",
      "[fold 1] train: 100.00, cv: 100.00, test: 100.00\n",
      "[fold 2] train: 100.00, cv: 100.00, test: 100.00\n",
      "[fold 3] train: 100.00, cv: 100.00, test: 100.00\n",
      "[fold 4] train: 100.00, cv: 100.00, test: 100.00\n",
      "[fold 5] train: 100.00, cv: 100.00, test: 100.00\n",
      "[fold 6] train: 100.00, cv: 100.00, test: 100.00\n",
      "[fold 7] train: 100.00, cv: 100.00, test: 100.00\n",
      "[fold 8] train: 100.00, cv: 100.00, test: 100.00\n",
      "[fold 9] train: 100.00, cv: 95.00, test: 100.00\n"
     ]
    }
   ],
   "source": [
    "# arrays to store model performance\n",
    "train_isof = []\n",
    "cv_isof = []\n",
    "test_isof = []\n",
    "\n",
    "# split training set into 10 part, each time the model is trained on 9 parts and cross validated on the remaining part\n",
    "for k, (train_index, test_index) in enumerate(kf.split(X_train)):\n",
    "    X, X_cv = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "    # sample weights\n",
    "    sample_weights = X['UBS offices']\n",
    "    # drop UBS offices column\n",
    "    X = X.drop(['UBS offices'], axis=1)\n",
    "    X_cv = X_cv.drop(['UBS offices'], axis=1)\n",
    "    # Demo model's sensitivity: modify weights\n",
    "    X['tot_inc'] = X.apply(lambda row: row.tot_inc * 2.0, axis=1)\n",
    "    # fit classifier on train set\n",
    "    iso_f.fit(X, sample_weight = sample_weights)\n",
    "    # prediction on train set, cross validation, and test set (either 1 or -1)\n",
    "    y_pred_train = iso_f.predict(X)\n",
    "    y_pred_cv = iso_f.predict(X_cv)\n",
    "    y_pred_test = iso_f.predict(X_test)\n",
    "    # count the number of wrong classifications\n",
    "    train_error = y_pred_train[y_pred_train == -1].size\n",
    "    cv_error = y_pred_cv[y_pred_cv == -1].size\n",
    "    test_error = y_pred_test[y_pred_test == -1].size\n",
    "    # model accuracy\n",
    "    train_accuracy = 1 - train_error / y_pred_train.size\n",
    "    cv_accuracy = 1 - cv_error / y_pred_cv.size\n",
    "    test_accuracy = 1 - test_error / y_pred_test.size\n",
    "    # print\n",
    "    train_isof.append(train_accuracy)\n",
    "    cv_isof.append(cv_accuracy)\n",
    "    test_isof.append(test_accuracy)\n",
    "    print(\"[fold {0}] train: {1:.2f}, cv: {2:.2f}, test: {3:.2f}\".\n",
    "          format(k, 100 * train_accuracy, 100 * cv_accuracy, 100 * test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The mean score and the 95% confidence interval of the score estimate are hence given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 100.00 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train accuracy: %0.2f (+/- %0.2f)\" % (100 * np.asarray(train_isof).mean(), 100 * np.asarray(train_isof).std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV accuracy: 99.50 (+/- 3.00)\n"
     ]
    }
   ],
   "source": [
    "print(\"CV accuracy: %0.2f (+/- %0.2f)\" % (100 * np.asarray(cv_isof).mean(), 100 * np.asarray(cv_isof).std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 100.00 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "print(\"Test accuracy: %0.2f (+/- %0.2f)\" % (100 * np.asarray(test_isof).mean(), 100 * np.asarray(test_isof).std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreeRegressor(criterion='mse', max_depth=8, max_features=1,\n",
      "          max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "          min_impurity_split=None, min_samples_leaf=1, min_samples_split=2,\n",
      "          min_weight_fraction_leaf=0.0, random_state=1952926171,\n",
      "          splitter='random')\n"
     ]
    }
   ],
   "source": [
    "# first tree\n",
    "dt = iso_f.estimators_[0]\n",
    "print(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save first tree structure\n",
    "# tree.export_graphviz(dt, out_file='dtree.dot', feature_names=X.columns, class_names=True, filled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os\\nos.environ[\"PATH\"] += os.pathsep + \\'C:/Users/Thang/Anaconda3/Library/bin/graphviz/\\'\\n\\ngraph = Source( tree.export_graphviz(dt, out_file=None, feature_names=X.columns, class_names=True, filled=True))\\npng_bytes = graph.pipe(format=\\'png\\')\\nwith open(\\'dtree.png\\',\\'wb\\') as f:\\n    f.write(png_bytes)\\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save as image\n",
    "'''\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Users/Thang/Anaconda3/Library/bin/graphviz/'\n",
    "\n",
    "graph = Source( tree.export_graphviz(dt, out_file=None, feature_names=X.columns, class_names=True, filled=True))\n",
    "png_bytes = graph.pipe(format='png')\n",
    "with open('dtree.png','wb') as f:\n",
    "    f.write(png_bytes)\n",
    "'''\n",
    "# from IPython.display import Image\n",
    "# Image(png_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ngraph = Source( tree.export_graphviz(dt, out_file=None, feature_names=X.columns, class_names=True, filled=True))\\nSVG(graph.pipe(format='svg'))\\n\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display in Jupyter Notebook\n",
    "'''\n",
    "graph = Source( tree.export_graphviz(dt, out_file=None, feature_names=X.columns, class_names=True, filled=True))\n",
    "SVG(graph.pipe(format='svg'))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Outlier Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# metric can be cityblock, cosine, euclidean, l1’, l2, manhattan\n",
    "lof = LocalOutlierFactor(n_neighbors=20, novelty=True, contamination= 0.001, metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fold 0] cv: 100.00, test: 100.00\n",
      "[fold 1] cv: 100.00, test: 100.00\n",
      "[fold 2] cv: 100.00, test: 100.00\n",
      "[fold 3] cv: 100.00, test: 100.00\n",
      "[fold 4] cv: 95.00, test: 100.00\n",
      "[fold 5] cv: 100.00, test: 100.00\n",
      "[fold 6] cv: 100.00, test: 100.00\n",
      "[fold 7] cv: 95.00, test: 100.00\n",
      "[fold 8] cv: 100.00, test: 100.00\n",
      "[fold 9] cv: 90.00, test: 100.00\n"
     ]
    }
   ],
   "source": [
    "# arrays to store model performance\n",
    "cv_lof = []\n",
    "test_lof = []\n",
    "\n",
    "# split training set into 10 part, each time the model is trained on 9 parts and cross validated on the remaining part\n",
    "for k, (train_index, test_index) in enumerate(kf.split(X_train)):\n",
    "    X, X_cv = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "    # drop UBS offices column\n",
    "    X = X.drop(['UBS offices'], axis=1)\n",
    "    X_cv = X_cv.drop(['UBS offices'], axis=1)\n",
    "    # scale to mean=0 and stdev=1 based on training set\n",
    "    scaler = preprocessing.StandardScaler().fit(X)\n",
    "    # apply scaling function on train, cross validation, and test set\n",
    "    X_train_n = scaler.transform(X)\n",
    "    X_cv_norm = scaler.transform(X_cv)\n",
    "    X_test_norm = scaler.transform(X_test)\n",
    "    # Demo model's sensitivity: modify weights\n",
    "    X['tot_inc'] = X.apply(lambda row: row.tot_inc * 2.0, axis=1)\n",
    "    # fit classifier on train set\n",
    "    lof.fit(X_train_n)\n",
    "    # prediction on cross validation and test set (either 1 or -1)\n",
    "    y_cv = lof.predict(X_cv_norm)\n",
    "    y_test = lof.predict(X_test_norm)\n",
    "    # count the number of wrong classifications\n",
    "    cv_error = y_cv[y_cv == -1].size\n",
    "    test_error = y_test[y_test == -1].size\n",
    "    # model accuracy\n",
    "    cv_accuracy = 1 - cv_error / y_cv.size\n",
    "    test_accuracy = 1 - test_error / y_test.size\n",
    "    # print\n",
    "    cv_lof.append(cv_accuracy)\n",
    "    test_lof.append(test_accuracy)\n",
    "    print(\"[fold {0}] cv: {1:.2f}, test: {2:.2f}\".\n",
    "          format(k, 100 * cv_accuracy, 100 * test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean score and the 95% confidence interval of the score estimate are hence given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV accuracy: 98.00 (+/- 6.63)\n"
     ]
    }
   ],
   "source": [
    "print(\"CV accuracy: %0.2f (+/- %0.2f)\" % (100 * np.asarray(cv_lof).mean(), 100 * np.asarray(cv_lof).std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 100.00 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "print(\"Test accuracy: %0.2f (+/- %0.2f)\" % (100 * np.asarray(test_lof).mean(), 100 * np.asarray(test_lof).std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create list of Zip codes that do not have UBS office\n",
    "prediction_set = df[~df.zipcode.isin(branch['Zip'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load zipcode csv\n",
    "zip_to_city = pd.read_csv(\"zip_code_database.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zip_to_city = zip_to_city[['zip', 'primary_city']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# join the two tables on zip code\n",
    "prediction_matrix = prediction_set.join(zip_to_city.set_index('zip'), on='zipcode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge City and State columns to create Location\n",
    "# cross_check = prediction_matrix\n",
    "# cross_check['Location'] = cross_check.apply (lambda row: row.primary_city + \", \" + row.STATE,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cross_check = cross_check.drop(['primary_city'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get a list of columns\n",
    "# cols = list(cross_check)\n",
    "\n",
    "# move the column to head of list using index, pop and insert\n",
    "# cols.insert(0, cols.pop(cols.index('Location')))\n",
    "\n",
    "# use loc to reorder\n",
    "# cross_check = cross_check.loc[:, cols]\n",
    "\n",
    "# save to csv\n",
    "# cross_check.to_csv('cross_check.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop irrelevant columns\n",
    "prediction_matrix = prediction_matrix.drop(['zipcode', 'STATEFIPS', 'agi_stub'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge City and State columns to create Location\n",
    "prediction_matrix['Location'] = prediction_matrix.apply (lambda row: row.primary_city + \", \" + row.STATE,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get a list of columns\n",
    "cols = list(prediction_matrix)\n",
    "\n",
    "# move the column to head of list using index, pop and insert\n",
    "cols.insert(0, cols.pop(cols.index('Location')))\n",
    "\n",
    "# use loc to reorder\n",
    "prediction_matrix = prediction_matrix.loc[:, cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop extra columns\n",
    "prediction_matrix = prediction_matrix.drop(['primary_city', 'STATE'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# group by Location and sum the stats\n",
    "group_predict = prediction_matrix.groupby('Location', as_index=False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save to csv\n",
    "# group_predict.to_csv('prediction_set.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# exclude Location that has 0 stats\n",
    "group_predict = group_predict[group_predict['N1'] != 0]\n",
    "group_predict = group_predict[group_predict['N1_2012'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create new columns\n",
    "group_predict['ave_div'] = group_predict.apply (lambda row: ave_div (row),axis=1)\n",
    "group_predict['ave_pro_income'] = group_predict.apply (lambda row: ave_pro_income (row),axis=1)\n",
    "group_predict['ave_self_retired'] = group_predict.apply (lambda row: ave_self_retired (row),axis=1)\n",
    "group_predict['ave_item_deduct'] = group_predict.apply (lambda row: ave_item_deduct (row),axis=1)\n",
    "group_predict['ave_RET'] = group_predict.apply (lambda row: ave_RET (row),axis=1)\n",
    "group_predict['ave_AMT'] = group_predict.apply (lambda row: ave_AMT (row),axis=1)\n",
    "group_predict['ave_EIC'] = group_predict.apply (lambda row: ave_EIC (row),axis=1)\n",
    "group_predict['ave_unemployment'] = group_predict.apply (lambda row: ave_unemployment (row),axis=1)\n",
    "group_predict['ave_edu_exp'] = group_predict.apply (lambda row: ave_edu_exp (row),axis=1)\n",
    "\n",
    "group_predict['tot_tax_cred'] = group_predict.apply (lambda row: tot_tax_cred (row),axis=1)\n",
    "group_predict['tot_inc'] = group_predict.apply (lambda row: tot_inc (row),axis=1)\n",
    "group_predict['tot_pens'] = group_predict.apply (lambda row: tot_pens (row),axis=1)\n",
    "group_predict['char_cont'] = group_predict.apply (lambda row: char_cont (row),axis=1)\n",
    "group_predict['net_inv'] = group_predict.apply (lambda row: net_inv (row),axis=1)\n",
    "group_predict['filings_change'] = group_predict.apply (lambda row: filings_change (row),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "group_predict = group_predict.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create new working table that only uses the selected features\n",
    "private_set = group_predict[['ave_div', 'ave_pro_income', 'ave_self_retired', 'ave_item_deduct',\n",
    "                             'ave_RET', 'ave_AMT', 'ave_EIC', 'ave_unemployment', 'ave_edu_exp',\n",
    "                             'tot_tax_cred', 'tot_inc', 'tot_pens', 'char_cont', 'net_inv', 'filings_change']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results from One-Class SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thang\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# scale to mean=0 and stdev=1 based on training set\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "\n",
    "# apply scaling function on prediction set\n",
    "X_predict = scaler.transform(private_set)\n",
    "\n",
    "# prediction on prediction set (either 1 or -1)\n",
    "ocsvm_prediction = ocsvm.predict(X_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Signed distance to the separating hyperplane (positive for an inlier and negative for an outlier)\n",
    "result = ocsvm.decision_function(X_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sort from largest to smallest distance\n",
    "ocsvm_array = sorted(result, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.035271529429985536,\n",
       " 0.031171973982380828,\n",
       " 0.030810462096946021,\n",
       " 0.030507116240225118,\n",
       " 0.030379213018699114,\n",
       " 0.030362403698022716,\n",
       " 0.029626612178356992,\n",
       " 0.029491174805242829,\n",
       " 0.029248571565102542,\n",
       " 0.02923466762881987]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 10\n",
    "ocsvm_pred = ocsvm_array[0:10]\n",
    "ocsvm_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# look up the index of matching distance in the original result array\n",
    "city_index = []\n",
    "\n",
    "for idx in ocsvm_pred:\n",
    "    item_index = np.where(result == idx)\n",
    "    for l in item_index:\n",
    "        city_index.extend(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lakeport, CA\n",
      "Venice, CA\n",
      "Milpitas, CA\n",
      "West Hollywood, CA\n",
      "Dublin, CA\n",
      "Trumansburg, NY\n",
      "Playa Del Rey, CA\n",
      "South San Francisco, CA\n",
      "Thorndale, TX\n",
      "Poteet, TX\n"
     ]
    }
   ],
   "source": [
    "# look up Location based on above indices\n",
    "ocsvm_res = []\n",
    "\n",
    "for res in city_index:\n",
    "    ocsvm_res.append(group_predict['Location'][res])\n",
    "    print(group_predict['Location'][res])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results from Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prediction on prediction set (either 1 or -1)\n",
    "isof_prediction = iso_f.predict(private_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Average anomaly score of X of the base classifiers (higher for normal, lower for outlier)\n",
    "isof_result = iso_f.decision_function(private_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sort from largest to smallest distance\n",
    "isof_array = sorted(isof_result, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.24927471996893436,\n",
       " 0.24748397262213306,\n",
       " 0.24683230855088834,\n",
       " 0.24667762645447205,\n",
       " 0.24613895586236395,\n",
       " 0.24612109122606823,\n",
       " 0.2459798109802982,\n",
       " 0.24506685911878467,\n",
       " 0.24469728986667971,\n",
       " 0.2446118941448534]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 10\n",
    "isof_pred = isof_array[0:10]\n",
    "isof_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# look up the index of matching distance in the original result array\n",
    "isof_city_index =[]\n",
    "\n",
    "for idx in isof_pred:\n",
    "    item_index = np.where(isof_result == idx)\n",
    "    for l in item_index:\n",
    "        isof_city_index.extend(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lewisburg, PA\n",
      "Manheim, PA\n",
      "Butte, MT\n",
      "Chippewa Falls, WI\n",
      "Michigan City, IN\n",
      "Greenwood, SC\n",
      "Versailles, KY\n",
      "Long Branch, NJ\n",
      "Newark, OH\n",
      "Great Falls, MT\n"
     ]
    }
   ],
   "source": [
    "# look up Location based on above indices\n",
    "iso_f_res = []\n",
    "\n",
    "for res in isof_city_index:\n",
    "    iso_f_res.append(group_predict['Location'][res])\n",
    "    print(group_predict['Location'][res])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results from Local Outlier Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thang\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# scale to mean=0 and stdev=1 based on training set\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "\n",
    "# apply scaling function on prediction set\n",
    "X_predict = scaler.transform(private_set)\n",
    "\n",
    "# prediction on prediction set (either 1 or -1)\n",
    "lof_prediction = lof.predict(X_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bigger is better, i.e. large values correspond to inliers.\n",
    "lof_result = lof.decision_function(X_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sort from largest to smallest distance\n",
    "lof_array = sorted(lof_result, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.0541312712398025,\n",
       " 2.0497191999436595,\n",
       " 2.048284564159327,\n",
       " 2.0454457212194819,\n",
       " 2.0445912565853366,\n",
       " 2.0444879859709832,\n",
       " 2.0420881400062552,\n",
       " 2.041885239714472,\n",
       " 2.0416908299287391,\n",
       " 2.0415795998818602]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 10\n",
    "lof_pred = lof_array[0:10]\n",
    "lof_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lof_city_index =[]\n",
    "\n",
    "for idx in lof_pred:\n",
    "    item_index = np.where(lof_result == idx)\n",
    "    for l in item_index:\n",
    "        lof_city_index.extend(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Huntingdon Valley, PA\n",
      "Hopkins, MN\n",
      "Zionsville, IN\n",
      "Evergreen, CO\n",
      "Powell, OH\n",
      "Butte, MT\n",
      "Cedar Rapids, IA\n",
      "Western Springs, IL\n",
      "Kennett Square, PA\n",
      "Lewisville, NC\n"
     ]
    }
   ],
   "source": [
    "lof_res = []\n",
    "\n",
    "for res in lof_city_index:\n",
    "    lof_res.append(group_predict['Location'][res])\n",
    "    print(group_predict['Location'][res])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_results = np.array([result, isof_result, lof_result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_results_df = pd.DataFrame({'OCSVM':combined_results[0,:],'ISOF':combined_results[1,:], 'LOF':combined_results[2,:]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = preprocessing.MinMaxScaler(feature_range=(0, len(combined_results_df))).fit(combined_results_df)\n",
    "combined_results_df_n = pd.DataFrame(scaler.transform(combined_results_df), columns = combined_results_df.columns, index = combined_results_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_results_df_n['combined'] = combined_results_df_n.apply (lambda row: row.OCSVM + row.ISOF + row.LOF,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_array = sorted(combined_results_df_n['combined'], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_pred = combined_array[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[29395.734883890931,\n",
       " 29338.939444744268,\n",
       " 29297.837187732031,\n",
       " 29297.73391589924,\n",
       " 29290.784378812888,\n",
       " 29203.379546450058,\n",
       " 29195.931940814855,\n",
       " 29188.43427283458,\n",
       " 29178.508001017406,\n",
       " 29177.055314281752]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comb_index =[]\n",
    "\n",
    "for rank in combined_pred:\n",
    "    idx = combined_results_df_n[combined_results_df_n['combined'] == rank].index.tolist()\n",
    "    comb_index.extend(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coralville, IA\n",
      "Covington, KY\n",
      "Long Branch, NJ\n",
      "Closter, NJ\n",
      "Carmel Valley, CA\n",
      "Delafield, WI\n",
      "Lewisburg, PA\n",
      "Butte, MT\n",
      "Chippewa Falls, WI\n",
      "Carpinteria, CA\n"
     ]
    }
   ],
   "source": [
    "comb_res = []\n",
    "\n",
    "for res in comb_index:\n",
    "    comb_res.append(group_predict['Location'][res])\n",
    "    print(group_predict['Location'][res])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load IRS csv\n",
    "# cc = pd.read_csv(\"cross_check.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross check statistics from OCSVM results\n",
    "# cc_ocsvm = cc[cc['Location'].isin(ocsvm_res)]\n",
    "# cc_ocsvm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross check statistics from Isolation Forest results\n",
    "# cc_isof = cc[cc['Location'].isin(iso_f_res)]\n",
    "# cc_isof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross check statistics from Local Outlier Factor results\n",
    "# cc_lof = cc[cc['Location'].isin(lof_res)]\n",
    "# cc_lof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross check statistics from Combined results\n",
    "# cc_comb = cc[cc['Location'].isin(comb_res)]\n",
    "# cc_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cc_ocsvm.to_csv('cc_ocsvm.csv', index=False)\n",
    "# cc_isof.to_csv('cc_isof.csv', index=False)\n",
    "# cc_lof.to_csv('cc_lof.csv', index=False)\n",
    "# cc_comb.to_csv('cc_comb.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
